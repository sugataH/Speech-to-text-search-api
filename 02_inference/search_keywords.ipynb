{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21925afd-c122-4a97-90b3-e805d5485824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 183 lines from corpus.\n",
      "Sample line: We show for the first time that learning powerful representations from speech\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# STEP 1: Load the corpus\n",
    "corpus_path = r\"C:\\voice_search_project_2\\01_data\\corpus.txt\"\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(corpus_lines)} lines from corpus.\")\n",
    "print(\"Sample line:\", corpus_lines[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c3167f-7b82-403e-9e83-9cf9230ce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Define stopwords (no NLTK required)\n",
    "stop_words = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\",\n",
    "    \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\",\n",
    "    \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n",
    "    \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "    \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "    \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
    "    \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "    \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "    \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "    \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\",\n",
    "    \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7548279-3549-458c-a877-08ab83d4b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Define search function using regex\n",
    "def search_corpus(transcription, corpus_lines, top_n=5):\n",
    "    # Extract keywords (remove stopwords)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', transcription.lower())\n",
    "    keywords = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if not keywords:\n",
    "        return []\n",
    "\n",
    "    # Count keyword matches in each line\n",
    "    matches = []\n",
    "    for line in corpus_lines:\n",
    "        line_lower = line.lower()\n",
    "        count = sum(1 for kw in keywords if re.search(rf\"\\b{re.escape(kw)}\\b\", line_lower))\n",
    "        if count > 0:\n",
    "            matches.append((line, count))\n",
    "\n",
    "    # Sort by match count\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [line for line, _ in matches[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7453b347-a710-4d24-8e70-a44ca6478473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "1. We show for the first time that learning powerful representations from speech\n",
      "2. listening to adults around them - a process that requires learning good representations of speech.\n",
      "3. Our results show that jointly learning discrete speech units with contextualized representations\n",
      "4. to speech recognition. The training accuracy of identifying the correct latent audio representation\n",
      "5. better results (see Appendix F for this experiment and other ablations on various hyperparameters). We presented wav2vec 2.0, a framework for self-supervised learning of speech representations which\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Test with an example transcription\n",
    "transcription = \"learning from speech representation\"\n",
    "results = search_corpus(transcription, corpus_lines)\n",
    "\n",
    "print(\"\\nSearch Results:\")\n",
    "for i, line in enumerate(results, 1):\n",
    "    print(f\"{i}. {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22c85d-398f-4dfe-8698-3de69094f0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
